## Maïwenn

Pour cette semaine, je me suis occupée de l'analyse avec stanza.

Tout d'abord, avant toute chose j'ai merge les fonctions load et save que nous avions réalisé sur main. Puis j'ai créé ma branche à partir de main. J'ai créé un script demo_stanza.py afin de tester un premier code. Ayant déjà manipulé stanza auparavant, je n'ai pas eu de mal à faire cela. J'ai donc ensuite créé le fichier analyzers.py et ait créé ma fonction d'analyse avec stanza. J'ai repris mon code de demo_stanza et je l'ai adapté pour qu'il prenne en argument un objet Article. J'ai aussi créé une fonction "load_model" afin de ne pas avoir à retélécharger le modèle si on l'a déjà téléchargé, ce qui sinon ralentit le code (déjà long). J'ai aussi créé une fonction main avec un parser afin d'ajouter des arguments pour exécuter le script depuis bash : ces arguments sont le fichier d'entrée qui est le fichier qui a été sauvegardé avec rss_parcours, le format du fichier d'entrée, l'analyzer à utiliser (stanza, trankit ou spacy) et enfin le format de sortie (json, pickle ou xml).
Dans datastructure j'ai ajouté une classe Token qui contient trois attributs : form, lemma et pos qui sont des strings. J'ai aussi créé une classe AnalyzedArticle qui contient l'article + le résultat de l'analyse c'est-à-dire la liste des objets Token. Le code analyse chaque article et stocke chaque AnalyzedArticle dans une liste qui est ensuite convertie en Corpus. Je n'ai pas encore pu modifier les fonctions de sauvegarde et de chargement pour prendre en compte les objets de type AnalyzedArticle car ces fonctions ne fonctionnent évidemment pas avec elles vu que ce ne sont pas les mêmes attributs que la classe Article. Il n'y a que pickle qui fonctionne car pickle dump tout le contenu de la variable directement dans le fichier, tandis que xml et json récupère chaque Article dans le Corpus.
